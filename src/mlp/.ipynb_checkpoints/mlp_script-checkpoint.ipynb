{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-15 23:58:46.303179: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-15 23:58:46.426764: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-15 23:58:46.426784: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-15 23:58:47.078747: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-15 23:58:47.078828: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-15 23:58:47.078836: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# set the seed to get reproducible results\n",
    "from black import out\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from keras.layers import Flatten, Dense, Dropout\n",
    "from keras import Sequential\n",
    "from keras.initializers import RandomNormal, HeNormal\n",
    "from keras.datasets import mnist\n",
    "from keras.regularizers import l2, l1\n",
    "import keras_tuner as kt\n",
    "import matplotlib.pyplot as plt\n",
    "import random as python_random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from pytictoc import TicToc  # time difference\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    os.environ[\"TF_CUDNN_DETERMINISTIC\"] = str(seed)\n",
    "\n",
    "    # source: https://keras.io/getting_started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\n",
    "    # source: https://github.com/keras-team/keras/issues/2743\n",
    "    np.random.seed(seed)\n",
    "    python_random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "\n",
    "#  load and normalize dataset\n",
    "(train_x, train_y), (test_x, test_y) = mnist.load_data()\n",
    "\n",
    "# vectorize image row-wise\n",
    "# shape = (num_samples, num_features)\n",
    "train_x = train_x.reshape(train_x.shape[0], -1)\n",
    "test_x = test_x.reshape(test_x.shape[0], -1)\n",
    "\n",
    "num_samples_training = train_x.shape[0]\n",
    "num_features = train_x.shape[1]\n",
    "num_classes = 10\n",
    "\n",
    "# memory efficient\n",
    "train_x = train_x.astype(\"float32\")\n",
    "test_x = test_x.astype(\"float32\")\n",
    "\n",
    "# min-max normalization\n",
    "train_x = train_x / 255\n",
    "test_x = test_x / 255\n",
    "\n",
    "# manual one-hot encoding instead of using 'sparse_categorical_entropy' (now 'categorical_crossentropy              ')\n",
    "# reason: train_y = keras.utils.to_categorical(train_y, num_classes)\n",
    "train_y_1hot = keras.utils.to_categorical(train_y, num_classes)\n",
    "test_y_1hot = keras.utils.to_categorical(test_y, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "# build the MLP\n",
    "# Notes:\n",
    "# regularization can be used in the output layer too, although in most examples they don't include it\n",
    "# dropout should not be used for input and output layers\n",
    "def create_model(\n",
    "    hidden_layer_nodes1=128,\n",
    "    hidden_layer_nodes2=256,\n",
    "    kernel_initializer=None,\n",
    "    kernel_regularizer=None,\n",
    "    is_dropout=False,\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"],\n",
    "):\n",
    "    hidden_layer_options = {}\n",
    "    output_layer_options = {}\n",
    "    if kernel_initializer:\n",
    "        hidden_layer_options[\"kernel_initializer\"] = kernel_initializer\n",
    "        output_layer_options[\"kernel_initializer\"] = kernel_initializer\n",
    "    if kernel_regularizer:\n",
    "        hidden_layer_options[\"kernel_regularizer\"] = kernel_regularizer\n",
    "\n",
    "    model = Sequential()\n",
    "    # 1st hidden layer\n",
    "    model.add(\n",
    "        Dense(\n",
    "            hidden_layer_nodes1,\n",
    "            input_shape=(num_features,),\n",
    "            activation=\"relu\",\n",
    "            **hidden_layer_options\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if is_dropout:\n",
    "        # source: https://machinelearningmastery.com/how-to-reduce-overfitting-with-dropout-regularization-in-keras/\n",
    "        model.add(Dropout(0.3))\n",
    "\n",
    "    # 2nd hidden layer\n",
    "    model.add(Dense(hidden_layer_nodes2, activation=\"relu\", **hidden_layer_options))\n",
    "\n",
    "    if is_dropout:\n",
    "        model.add(Dropout(0.3))\n",
    "\n",
    "    # output\n",
    "    model.add(Dense(num_classes, activation=\"softmax\", **output_layer_options))\n",
    "    # model.add(Dense(num_classes, activation=\"softmax\", **hidden_layer_options))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=metrics,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def plot_weights(weight):\n",
    "    plt.figure(constrained_layout=True)\n",
    "    plt.subplot(131)\n",
    "    sns.violinplot(y=weight[0], color=\"b\")\n",
    "    plt.xlabel(\"Hidden Layer 1\")\n",
    "    plt.subplot(132)\n",
    "    sns.violinplot(y=weight[1], color=\"y\")\n",
    "    plt.xlabel(\"Hidden Layer 2\")\n",
    "    plt.subplot(133)\n",
    "    sns.violinplot(y=weight[2], color=\"r\")\n",
    "    plt.xlabel(\"Output\")\n",
    "\n",
    "\n",
    "def filter_weights(model):\n",
    "    weights = [\n",
    "        model.get_weights()[0].flatten().reshape(-1, 1),  # hidden layer 1\n",
    "        model.get_weights()[2].flatten().reshape(-1, 1),  # hidden layer 2\n",
    "        model.get_weights()[4].flatten().reshape(-1, 1),  # output\n",
    "    ]\n",
    "    return weights\n",
    "\n",
    "\n",
    "def fitWrapper(batch_size, epochs):\n",
    "    history = model.fit(\n",
    "        train_x,\n",
    "        train_y_1hot,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_split=0.2,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    weights = filter_weights(model)\n",
    "    return history, weights\n",
    "\n",
    "\n",
    "# plot learning curves\n",
    "def plot_history(history):\n",
    "    epochs = len(history.history[\"accuracy\"])\n",
    "    x = np.arange(1, epochs + 1)\n",
    "    plt.figure(constrained_layout=True)\n",
    "    plt.subplot(211)\n",
    "    plt.plot(x, history.history[\"accuracy\"])\n",
    "    plt.plot(x, history.history[\"val_accuracy\"], color=\"green\")\n",
    "    # plt.xlabel(\"epochs\")\n",
    "    # plt.ylabel(\"accuracy\")\n",
    "    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
    "\n",
    "    plt.subplot(212)\n",
    "    plt.plot(x, history.history[\"loss\"])\n",
    "    plt.plot(x, history.history[\"val_loss\"], color=\"green\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.legend([\"train\", \"validation\"], loc=\"upper right\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-15 23:59:10.071357: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-15 23:59:10.071427: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-15 23:59:10.071477: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (thodkatz-hp): /proc/driver/nvidia/version does not exist\n",
      "2023-02-15 23:59:10.072049: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-15 23:59:10.180754: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 150528000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/thodkatz/repos/personal/ece-ai/src/mlp/venv/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/thodkatz/repos/personal/ece-ai/src/mlp/venv/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/thodkatz/repos/personal/ece-ai/src/mlp/venv/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/thodkatz/repos/personal/ece-ai/src/mlp/venv/lib/python3.10/site-packages/keras/engine/training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/thodkatz/repos/personal/ece-ai/src/mlp/venv/lib/python3.10/site-packages/keras/engine/training.py\", line 1082, in compute_loss\n        return self.compiled_loss(\n    File \"/home/thodkatz/repos/personal/ece-ai/src/mlp/venv/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/thodkatz/repos/personal/ece-ai/src/mlp/venv/lib/python3.10/site-packages/keras/losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/thodkatz/repos/personal/ece-ai/src/mlp/venv/lib/python3.10/site-packages/keras/losses.py\", line 284, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/thodkatz/repos/personal/ece-ai/src/mlp/venv/lib/python3.10/site-packages/keras/losses.py\", line 2004, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/home/thodkatz/repos/personal/ece-ai/src/mlp/venv/lib/python3.10/site-packages/keras/backend.py\", line 5532, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (1, 1) and (1, 10) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m batches:\n\u001b[1;32m      4\u001b[0m     model \u001b[38;5;241m=\u001b[39m create_model()\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/personal/ece-ai/src/mlp/venv/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileqk3k3jx6.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/thodkatz/repos/personal/ece-ai/src/mlp/venv/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/thodkatz/repos/personal/ece-ai/src/mlp/venv/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/thodkatz/repos/personal/ece-ai/src/mlp/venv/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/thodkatz/repos/personal/ece-ai/src/mlp/venv/lib/python3.10/site-packages/keras/engine/training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/thodkatz/repos/personal/ece-ai/src/mlp/venv/lib/python3.10/site-packages/keras/engine/training.py\", line 1082, in compute_loss\n        return self.compiled_loss(\n    File \"/home/thodkatz/repos/personal/ece-ai/src/mlp/venv/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/thodkatz/repos/personal/ece-ai/src/mlp/venv/lib/python3.10/site-packages/keras/losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/thodkatz/repos/personal/ece-ai/src/mlp/venv/lib/python3.10/site-packages/keras/losses.py\", line 284, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/thodkatz/repos/personal/ece-ai/src/mlp/venv/lib/python3.10/site-packages/keras/losses.py\", line 2004, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/home/thodkatz/repos/personal/ece-ai/src/mlp/venv/lib/python3.10/site-packages/keras/backend.py\", line 5532, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (1, 1) and (1, 10) are incompatible\n"
     ]
    }
   ],
   "source": [
    "# copy-paste thing, not intended to be executed right?? I just used it for the report\n",
    "batches = [1, 256, num_samples_training]\n",
    "for batch in batches:\n",
    "    model = create_model()\n",
    "    model.fit(train_x, train_y, batch_size=batch, epochs=100, validation_split=0.2, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Batch size: 1\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-16 00:02:22.719797: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 150528000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19535/48000 [===========>..................] - ETA: 1:12 - loss: 0.3172 - accuracy: 0.9063"
     ]
    }
   ],
   "source": [
    "# default network for different batch sizes\n",
    "batches = [1, 256, num_samples_training]\n",
    "for batch in batches:\n",
    "    set_seed(1)  # get reproducible results\n",
    "    print(\"\\n\\nBatch size: \" + str(batch))\n",
    "    # default optimizer adam\n",
    "    model = create_model()\n",
    "    weights = filter_weights(model)\n",
    "    plot_weights(weights)\n",
    "\n",
    "    t = TicToc()\n",
    "    t.tic()\n",
    "    history, weight = fitWrapper(batch_size=batch, epochs=100)\n",
    "    t.toc()\n",
    "\n",
    "    result = model.evaluate(test_x, test_y_1hot)\n",
    "    print(\"Accuracy: \" + str(result))\n",
    "    plot_weights(weight)\n",
    "    plot_history(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sgd + custom weight initialization\n",
    "set_seed(1)\n",
    "\n",
    "model = create_model(optimizer=\"sgd\", kernel_initializer=RandomNormal(mean=10))\n",
    "weights = filter_weights(model)\n",
    "plot_weights(weights)\n",
    "\n",
    "history, weights = fitWrapper(batch_size=256, epochs=100)\n",
    "\n",
    "weights = filter_weights(model)\n",
    "plot_weights(weights)\n",
    "plot_history(history)\n",
    "model.evaluate(test_x, test_y_1hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l2 regularization model\n",
    "set_seed(1)\n",
    "\n",
    "model = create_model(\n",
    "    optimizer=RMSprop(learning_rate=0.001, rho=0.9), kernel_regularizer=l2(0.1)\n",
    ")\n",
    "weights = filter_weights(model)\n",
    "plot_weights(weights)\n",
    "\n",
    "history, weights = fitWrapper(batch_size=256, epochs=100)\n",
    "\n",
    "weights = filter_weights(model)\n",
    "plot_weights(weights)\n",
    "plot_history(history)\n",
    "model.evaluate(test_x, test_y_1hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l1-dropout regularization\n",
    "set_seed(1)\n",
    "\n",
    "model = create_model(\n",
    "    optimizer=RMSprop(learning_rate=0.001, rho=0.9),\n",
    "    kernel_regularizer=l1(0.01),\n",
    "    is_dropout=True,\n",
    ")\n",
    "weights = filter_weights(model)\n",
    "plot_weights(weights)\n",
    "\n",
    "history, weights = fitWrapper(batch_size=256, epochs=100)\n",
    "\n",
    "weights = filter_weights(model)\n",
    "plot_weights(weights)\n",
    "plot_history(history)\n",
    "model.evaluate(test_x, test_y_1hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Fine-tuning\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "set_seed(1)\n",
    "\n",
    "# custom metric functions\n",
    "# source: https://github.com/keras-team/autokeras/issues/867#issuecomment-664794336\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "    hidden_layer_nodes1 = hp.Choice(\"hidden_layer_nodes1\", values=[64, 128])\n",
    "    hidden_layer_nodes2 = hp.Choice(\"hidden_layer_nodes2\", values=[256, 512])\n",
    "    learning_rate = hp.Choice(\"learning_rate\", values=[0.1, 0.01, 0.001])\n",
    "    l2_alpha = hp.Choice(\"l2_alpha\", values=[0.1, 0.001, 0.000001])\n",
    "    return create_model(\n",
    "        hidden_layer_nodes1=hidden_layer_nodes1,\n",
    "        hidden_layer_nodes2=hidden_layer_nodes2,\n",
    "        kernel_regularizer=l2(l2_alpha),\n",
    "        kernel_initializer=HeNormal(),\n",
    "        optimizer=RMSprop(learning_rate=learning_rate),\n",
    "        metrics=[\"accuracy\", f1_score, recall_m, precision_m],\n",
    "    )\n",
    "\n",
    "\n",
    "build_model(kt.HyperParameters())\n",
    "\n",
    "tuner = kt.Hyperband(hypermodel=build_model, objective=\"val_accuracy\")\n",
    "tuner.search(\n",
    "    train_x,\n",
    "    train_y_1hot,\n",
    "    validation_split=0.2,\n",
    "    epochs=1000,\n",
    "    callbacks=[EarlyStopping(patience=200, monitor=\"val_loss\")],\n",
    ")\n",
    "tuner.results_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "history = best_model.fit(train_x, train_y_1hot, epochs=10, validation_split=0.2)\n",
    "loss_val, accuracy_val, f1_score_val, recall_val, precision_val = best_model.evaluate(\n",
    "    test_x, test_y_1hot\n",
    ")\n",
    "\n",
    "print(\"loss:\" + str(loss_val))\n",
    "print(\"accuracy:\" + str(accuracy_val))\n",
    "print(\"f1 score:\" + str(f1_score_val))\n",
    "print(\"recall:\" + str(recall_val))\n",
    "print(\"precision:\" + str(precision_val))\n",
    "\n",
    "# learning curves\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = best_model.predict(test_x)\n",
    "confusion_mat = confusion_matrix(test_y, y_pred.argmax(axis=1))\n",
    "normed_conf = (confusion_mat.T / confusion_mat.astype(float).sum(axis=1)).T\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.heatmap(normed_conf, annot=True, fmt=\".2f\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.xlabel(\"Predicted\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
