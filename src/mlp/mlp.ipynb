{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# set the seed to get reproducible results\n",
    "from black import out\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from keras_tuner import Objective\n",
    "from keras.layers import Flatten, Dense, Dropout\n",
    "from keras import Sequential\n",
    "from keras.initializers import RandomNormal, HeNormal\n",
    "from keras.datasets import mnist\n",
    "from keras.regularizers import l2, l1\n",
    "from keras.optimizers import SGD\n",
    "import keras_tuner as kt\n",
    "import matplotlib.pyplot as plt\n",
    "import random as python_random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from pytictoc import TicToc  # time difference\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    os.environ[\"TF_CUDNN_DETERMINISTIC\"] = str(seed)\n",
    "\n",
    "    # source: https://keras.io/getting_started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\n",
    "    # source: https://github.com/keras-team/keras/issues/2743\n",
    "    np.random.seed(seed)\n",
    "    python_random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "\n",
    "#  load and normalize dataset\n",
    "(train_x, train_y), (test_x, test_y) = mnist.load_data()\n",
    "\n",
    "# vectorize image row-wise\n",
    "# shape = (num_samples, num_features)\n",
    "train_x = train_x.reshape(train_x.shape[0], -1)\n",
    "test_x = test_x.reshape(test_x.shape[0], -1)\n",
    "\n",
    "num_samples_training = train_x.shape[0]\n",
    "num_features = train_x.shape[1]\n",
    "num_classes = 10\n",
    "\n",
    "# memory efficient\n",
    "train_x = train_x.astype(\"float32\")\n",
    "test_x = test_x.astype(\"float32\")\n",
    "\n",
    "# min-max normalization\n",
    "train_x = train_x / 255\n",
    "test_x = test_x / 255\n",
    "\n",
    "# manual one-hot encoding instead of using 'sparse_categorical_entropy' (now 'categorical_crossentropy              ')\n",
    "# reason: https://stackoverflow.com/questions/49019383/keras-precision-and-recall-is-greater-than-1-multi-classification\n",
    "train_y_1hot = keras.utils.to_categorical(train_y, num_classes)\n",
    "test_y_1hot = keras.utils.to_categorical(test_y, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "# build the MLP\n",
    "# Notes:\n",
    "# regularization can be used in the output layer too, although in most examples they don't include it\n",
    "# dropout should not be used for input and output layers\n",
    "def create_model(\n",
    "    hidden_layer_nodes1=128,\n",
    "    hidden_layer_nodes2=256,\n",
    "    kernel_initializer=None,\n",
    "    kernel_regularizer=None,\n",
    "    is_dropout=False,\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"],\n",
    "):\n",
    "    hidden_layer_options = {}\n",
    "    output_layer_options = {}\n",
    "    if kernel_initializer:\n",
    "        hidden_layer_options[\"kernel_initializer\"] = kernel_initializer\n",
    "        output_layer_options[\"kernel_initializer\"] = kernel_initializer\n",
    "    if kernel_regularizer:\n",
    "        hidden_layer_options[\"kernel_regularizer\"] = kernel_regularizer\n",
    "\n",
    "    model = Sequential()\n",
    "    # 1st hidden layer\n",
    "    model.add(Dense(hidden_layer_nodes1, input_shape=(num_features,), activation=\"relu\", **hidden_layer_options))\n",
    "\n",
    "    if is_dropout:\n",
    "        # source: https://machinelearningmastery.com/how-to-reduce-overfitting-with-dropout-regularization-in-keras/\n",
    "        model.add(Dropout(0.3))\n",
    "\n",
    "    # 2nd hidden layer\n",
    "    model.add(Dense(hidden_layer_nodes2, activation=\"relu\", **hidden_layer_options))\n",
    "\n",
    "    if is_dropout:\n",
    "        model.add(Dropout(0.3))\n",
    "\n",
    "    # output\n",
    "    model.add(Dense(num_classes, activation=\"softmax\", **output_layer_options))\n",
    "    # model.add(Dense(num_classes, activation=\"softmax\", **hidden_layer_options))\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=metrics)\n",
    "    return model\n",
    "\n",
    "\n",
    "def plot_weights(weight):\n",
    "    plt.figure(constrained_layout=True)\n",
    "    plt.subplot(131)\n",
    "    sns.violinplot(y=weight[0], color=\"b\")\n",
    "    plt.xlabel(\"Hidden Layer 1\")\n",
    "    plt.subplot(132)\n",
    "    sns.violinplot(y=weight[1], color=\"y\")\n",
    "    plt.xlabel(\"Hidden Layer 2\")\n",
    "    plt.subplot(133)\n",
    "    sns.violinplot(y=weight[2], color=\"r\")\n",
    "    plt.xlabel(\"Output\")\n",
    "\n",
    "\n",
    "def filter_weights(model):\n",
    "    weights = [\n",
    "        model.get_weights()[0].flatten().reshape(-1, 1),  # hidden layer 1\n",
    "        model.get_weights()[2].flatten().reshape(-1, 1),  # hidden layer 2\n",
    "        model.get_weights()[4].flatten().reshape(-1, 1),  # output\n",
    "    ]\n",
    "    return weights\n",
    "\n",
    "\n",
    "def fitWrapper(batch_size, epochs):\n",
    "    history = model.fit(\n",
    "        train_x,\n",
    "        train_y_1hot,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_split=0.2,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    weights = filter_weights(model)\n",
    "    return history, weights\n",
    "\n",
    "\n",
    "# plot learning curves\n",
    "def plot_history(history):\n",
    "    epochs = len(history.history[\"accuracy\"])\n",
    "    x = np.arange(1, epochs + 1)\n",
    "    plt.figure(constrained_layout=True)\n",
    "    plt.subplot(211)\n",
    "    plt.plot(x, history.history[\"accuracy\"])\n",
    "    plt.plot(x, history.history[\"val_accuracy\"], color=\"green\")\n",
    "    # plt.xlabel(\"epochs\")\n",
    "    # plt.ylabel(\"accuracy\")\n",
    "    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
    "\n",
    "    plt.subplot(212)\n",
    "    plt.plot(x, history.history[\"loss\"])\n",
    "    plt.plot(x, history.history[\"val_loss\"], color=\"green\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.legend([\"train\", \"validation\"], loc=\"upper right\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default network for different batch sizes\n",
    "batches = [1, 256,num_samples_training]\n",
    "for batch in batches:\n",
    "    set_seed(1)  # get reproducible results\n",
    "    print(\"\\n\\nBatch size: \" + str(batch))\n",
    "    # default optimizer adam\n",
    "    model = create_model()\n",
    "    weights = filter_weights(model)\n",
    "    plot_weights(weights)\n",
    "\n",
    "    t = TicToc()\n",
    "    t.tic()\n",
    "    history, weight = fitWrapper(batch_size=batch, epochs=100)\n",
    "    t.toc()\n",
    "\n",
    "    result = model.evaluate(test_x, test_y_1hot)\n",
    "    print(\"Accuracy: \" + str(result))\n",
    "    plot_weights(weight)\n",
    "    plot_history(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmsprop \n",
    "\n",
    "rhos = [0.01, 0.99]\n",
    "for rho in rhos:\n",
    "    set_seed(1)\n",
    "\n",
    "    model = create_model(optimizer=RMSprop(learning_rate=0.001, rho=rho))\n",
    "    weights = filter_weights(model)\n",
    "    plot_weights(weights)\n",
    "\n",
    "    history, weights = fitWrapper(batch_size=256, epochs=100)\n",
    "\n",
    "    weights = filter_weights(model)\n",
    "    plot_weights(weights)\n",
    "    plot_history(history)\n",
    "    print(\"Evalute\", model.evaluate(test_x, test_y_1hot))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sgd + weight initialization\n",
    "set_seed(1)\n",
    "\n",
    "model = create_model(\n",
    "    optimizer=SGD(lr=0.01), \n",
    "    kernel_initializer=RandomNormal(mean=10))\n",
    "weights = filter_weights(model)\n",
    "plot_weights(weights)\n",
    "\n",
    "history, weights = fitWrapper(batch_size=256, epochs=100)\n",
    "\n",
    "weights = filter_weights(model)\n",
    "plot_weights(weights)\n",
    "plot_history(history)\n",
    "print(\"Evalute\", model.evaluate(test_x, test_y_1hot))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l2 regularization model + rmsprop\n",
    "alphas = [0.1, 0.01, 0.001]\n",
    "for alpha in alphas:\n",
    "    set_seed(1)\n",
    "\n",
    "    model = create_model(\n",
    "        optimizer=RMSprop(learning_rate=0.001, rho=0.99), \n",
    "        kernel_regularizer=l2(alpha)\n",
    "    )\n",
    "    weights = filter_weights(model)\n",
    "    plot_weights(weights)\n",
    "\n",
    "    history, weights = fitWrapper(batch_size=256, epochs=100)\n",
    "\n",
    "    weights = filter_weights(model)\n",
    "    plot_weights(weights)\n",
    "    plot_history(history)\n",
    "    print(\"Evaluate\", alpha, model.evaluate(test_x, test_y_1hot))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l2 regularization model + sgd\n",
    "alphas = [0.1, 0.01, 0.001]\n",
    "for alpha in alphas:\n",
    "    set_seed(1)\n",
    "\n",
    "    model = create_model(\n",
    "        optimizer=SGD(lr=0.01), \n",
    "        kernel_initializer=RandomNormal(10),  \n",
    "        kernel_regularizer=l2(alpha))\n",
    "    weights = filter_weights(model)\n",
    "    plot_weights(weights)\n",
    "\n",
    "    history, weights = fitWrapper(batch_size=256, epochs=100)\n",
    "\n",
    "    weights = filter_weights(model)\n",
    "    plot_weights(weights)\n",
    "    plot_history(history)\n",
    "    print(\"Evaluate\", alpha, model.evaluate(test_x, test_y_1hot))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l1-dropout regularization rmsprop\n",
    "set_seed(1)\n",
    "\n",
    "model = create_model(\n",
    "    optimizer=RMSprop(learning_rate=0.001, rho=0.99),\n",
    "    kernel_regularizer=l1(0.01),\n",
    "    is_dropout=True,\n",
    ")\n",
    "weights = filter_weights(model)\n",
    "plot_weights(weights)\n",
    "\n",
    "history, weights = fitWrapper(batch_size=256, epochs=100)\n",
    "\n",
    "weights = filter_weights(model)\n",
    "plot_weights(weights)\n",
    "plot_history(history)\n",
    "print(\"Evalute\", model.evaluate(test_x, test_y_1hot))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l1-dropout regularization sgd + initialization\n",
    "set_seed(1)\n",
    "\n",
    "model = create_model(\n",
    "    optimizer=SGD(lr=0.01),\n",
    "    kernel_initializer=RandomNormal(10),\n",
    "    kernel_regularizer=l1(0.01),\n",
    "    is_dropout=True,\n",
    ")\n",
    "weights = filter_weights(model)\n",
    "plot_weights(weights)\n",
    "\n",
    "history, weights = fitWrapper(batch_size=256, epochs=100)\n",
    "\n",
    "weights = filter_weights(model)\n",
    "plot_weights(weights)\n",
    "plot_history(history)\n",
    "print(\"Evalute\", model.evaluate(test_x, test_y_1hot))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Fine-tuning\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "set_seed(1)\n",
    "\n",
    "# custom metric functions\n",
    "# source: https://github.com/keras-team/autokeras/issues/867#issuecomment-664794336\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "    hidden_layer_nodes1 = hp.Choice(\"hidden_layer_nodes1\", values=[64, 128])\n",
    "    hidden_layer_nodes2 = hp.Choice(\"hidden_layer_nodes2\", values=[256, 512])\n",
    "    learning_rate = hp.Choice(\"learning_rate\", values=[0.1, 0.01, 0.001])\n",
    "    l2_alpha = hp.Choice(\"l2_alpha\", values=[0.1, 0.001, 0.000001])\n",
    "    return create_model(\n",
    "        hidden_layer_nodes1=hidden_layer_nodes1,\n",
    "        hidden_layer_nodes2=hidden_layer_nodes2,\n",
    "        kernel_regularizer=l2(l2_alpha),\n",
    "        kernel_initializer=HeNormal(),\n",
    "        optimizer=RMSprop(learning_rate=learning_rate),\n",
    "        metrics=[\"accuracy\", f1_score, recall_m, precision_m],\n",
    "    )\n",
    "\n",
    "\n",
    "build_model(kt.HyperParameters())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose your HyperBand\n",
    "\n",
    "Objective can be:\n",
    "- `val_f1_score`\n",
    "- `val_accuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# https://neptune.ai/blog/keras-tuner-tuning-hyperparameters-deep-learning-model\n",
    "# we could use `val_f1`` ? But we will have bad results?\n",
    "# `val_f1` isn't supported but you can define cusotm f1 function\n",
    "# https://github.com/keras-team/autokeras/issues/867\n",
    "\n",
    "\n",
    "tuner = kt.Hyperband(hypermodel=build_model, objective=Objective(\"val_f1_score\", direction='max'))\n",
    "tuner.search(\n",
    "    train_x,\n",
    "    train_y_1hot,\n",
    "    validation_split=0.2,\n",
    "    epochs=1000,\n",
    "    callbacks=[EarlyStopping(patience=200, monitor=\"val_loss\")],\n",
    ")\n",
    "tuner.results_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tuner = kt.Hyperband(hypermodel=build_model, objective=\"val_accuracy\")\n",
    "tuner.search(\n",
    "    train_x,\n",
    "    train_y_1hot,\n",
    "    validation_split=0.2,\n",
    "    epochs=1000,\n",
    "    callbacks=[EarlyStopping(patience=200, monitor=\"val_loss\")],\n",
    ")\n",
    "tuner.results_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train with the best hyperparameters\n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "history = best_model.fit(train_x, train_y_1hot, epochs=50, validation_split=0.2)\n",
    "loss_val, accuracy_val, f1_score_val, recall_val, precision_val = best_model.evaluate(\n",
    "    test_x, test_y_1hot\n",
    ")\n",
    "\n",
    "print(\"loss:\" + str(loss_val))\n",
    "print(\"accuracy:\" + str(accuracy_val))\n",
    "print(\"f1 score:\" + str(f1_score_val))\n",
    "print(\"recall:\" + str(recall_val))\n",
    "print(\"precision:\" + str(precision_val))\n",
    "\n",
    "# learning curves\n",
    "plot_history(history)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# this one is already trained by getting the best model and we can observe overfitting just from the start\n",
    "# we deduce that by the starting training accuracy of 1.0\n",
    "# it is better to create the model by the hyperparameters\n",
    "\n",
    "# train, test the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "history = best_model.fit(train_x, train_y_1hot, epochs=10, validation_split=0.2)\n",
    "loss_val, accuracy_val, f1_score_val, recall_val, precision_val = best_model.evaluate(\n",
    "    test_x, test_y_1hot\n",
    ")\n",
    "\n",
    "print(\"loss:\" + str(loss_val))\n",
    "print(\"accuracy:\" + str(accuracy_val))\n",
    "print(\"f1 score:\" + str(f1_score_val))\n",
    "print(\"recall:\" + str(recall_val))\n",
    "print(\"precision:\" + str(precision_val))\n",
    "\n",
    "# learning curves\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = best_model.predict(test_x)\n",
    "confusion_mat = confusion_matrix(test_y, y_pred.argmax(axis=1))\n",
    "normed_conf = (confusion_mat.T / confusion_mat.astype(float).sum(axis=1)).T\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.heatmap(normed_conf, annot=True, fmt=\".2f\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.xlabel(\"Predicted\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ebf04c758871b1ddcbe87493a0c8c398f718eecf6b14bc863ee76a06d2b21aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
